# attention functions

def scale_dot_product_attention():
    pass

def multi_head_attention():
    pass

def encoder_decoder_attention():
    pass